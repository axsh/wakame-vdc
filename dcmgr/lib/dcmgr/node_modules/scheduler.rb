# -*- coding: utf-8 -*-
require 'isono'

module Dcmgr
  module NodeModules
    class Scheduler < Isono::NodeModules::Base
      include Dcmgr::Logger
      include Dcmgr::VNet::VNicInitializer
      include Dcmgr::VNet::CallIsonoPacketfilter

      # module_module: bin/collector
      initialize_hook do
        app = Isono::Rack::ObjectMethod.new(myinstance)
        job = Isono::NodeModules::JobChannel.new(node)
        job.register_endpoint('scheduler', Isono::Rack.build do
                                use Isono::Rack::Sequel
                                run proc { |req, res|
                                  Thread.current[Models::BaseNew::LOCK_TABLES_KEY] = {}
                                  app.call(req, res)
                                }
                              end)
      end

      terminate_hook do
      end

      def schedule_instance(instance_id)
        instance = Models::Instance[instance_id] || raise("Unknown instance: #{instance_id}")

        db.after_rollback do
          if instance
            db.transaction do
              instance.refresh.destroy
              # volumes are still in available state. so bring them to deleted.
              instance.volumes_dataset.alives.all.each(&:destroy)
            end
          end
        end

        svc_type = Dcmgr::Scheduler.service_type(instance)

        svc_type.host_node.schedule(instance) if instance.host_node.nil?
        svc_type.network.schedule(instance) if instance.network_vif.empty?
        init_security_groups(instance)
        instance.save_changes
        setup_vif_monitors(instance)

        instance.state = Constants::Instance::STATE_PENDING
        instance.save_changes
        logger.info "%s is located at %s" %
          [instance.canonical_uuid, instance.host_node.canonical_uuid]

        instance.volumes_dataset.filter_by_state(Const::Volume::STATE_SCHEDULING).each { |vol|
          logger.info "Scheduling volume devices for %s: %s" %
            [instance.canonical_uuid, vol.canonical_uuid]

          case vol.volume_type
          when Dcmgr::Models::LocalVolume.to_s
            if instance.host_node.available_disk_space < vol.size
              raise Dcmgr::Scheduler::HostNodeSchedulingError,
                    "Insufficient local disk space #{vol.canonical_uuid}."
            end
            vol.volume_class.create(:id=>vol.id,
                                    :instance_id=>instance.pk,
                                    :mount_label=>'instance')
          when nil
            svc_type.storage_node.schedule(vol)
          else
            raise "Unknown Volume Type: #{vol.volume_type}"
          end

          vol.state = Constants::Volume::STATE_PENDING
          vol.save_changes

          logger.info "%s is a %s located at %s" %
            [vol.canonical_uuid, vol.volume_type, vol.storage_node.canonical_uuid]
        }

        instance.volumes_dataset.filter_by_state(Const::Volume::STATE_PENDING).each { |vol|
          case vol.volume_type
          when Dcmgr::Models::LocalVolume.to_s
          else
            db.after_commit do
              self.job.submit("sta-handle.#{vol.volume_device.storage_node.node_id}",
                              'create_volume_and_run_instance', vol.canonical_uuid, instance.canonical_uuid)
            end
          end
        }

        case instance.image.boot_dev_type
        when Const::Image::BOOT_DEV_LOCAL
          db.after_commit do
            self.job.submit("local-store-handle.#{instance.host_node.node_id}",
                            'run_local_store', instance.canonical_uuid)
          end
        when Const::Image::BOOT_DEV_SAN
          # do nothing
        else
          raise "Unknown boot type"
        end

        db.after_commit do
          event.publish('instance.scheduled', :args=>[instance.canonical_uuid])
        end

        logger.info("Provisioned instance: #{instance.canonical_uuid} on #{instance.host_node.canonical_uuid}")
        logger.info("Finished scheduling instance [inst_id: #{instance_id}]")
        nil
      rescue ::Exception => e
        logger.error("Error occured. schedule_instance(inst_id: #{instance_id})")
        logger.error(e)
        instance.refresh.destroy if instance

        raise e
      end

      def schedule_start_instance(instance_id)
        instance = Models::Instance[instance_id]
        vol = instance.boot_volume
        svc_type = Dcmgr::Scheduler.service_type(instance)

        svc_type.host_node.schedule(instance)
        instance.nic.each { |nic|
          nic.lease_ip_lease
        }
        instance.save_changes

        init_security_groups(instance)
        instance.state = :pending
        instance.save

        commit_transaction
        case instance.image.boot_dev_type
        when Const::Image::BOOT_DEV_SAN
          self.job.submit("hva-handle.#{instance.host_node.node_id}", 'run_vol_store', instance.canonical_uuid, vol.canonical_uuid)
        when Const::Image::BOOT_DEV_LOCAL
          self.job.submit("local-store-handle.#{instance.host_node.node_id}", 'run_local_store', instance.canonical_uuid)
        else
          raise "Unknown boot type"
        end
        event.publish('instance.scheduled', :args=>[instance.canonical_uuid])

      rescue ::Exception => e
        rollback_transaction rescue nil

        logger.error("Error occured. [inst_id: #{instance_id}]")
        logger.error(e)
        instance.refresh.destroy if instance
        vol.refresh.destroy if vol
        return
      end

      def schedule_volume(volume_id)
        volume = Models::Volume[volume_id] || raise("Unknown volume ID: #{volume_id}")
        svc_type = Dcmgr::Scheduler.service_type(volume)

        svc_type.storage_node.schedule(volume)
        volume.save

        volume.state = :pending
        volume.save

        commit_transaction

        self.job.submit("sta-handle.#{volume.storage_node.node_id}",
                        'create_volume', volume.canonical_uuid)
      rescue ::Exception => e
        rollback_transaction rescue nil

        logger.error("Error occured. [volume_id: #{volume_id}]")
        logger.error(e)
        logger.error(e.backtrace.join("\n"))
        volume.refresh.destroy
      end

      def evacuate_from(host_node_id, keep_node_offline=false)
        src_host_node = Models::HostNode[host_node_id] || raise("Unknown host node: #{host_node_id}")

        begin
          # mark the host node as unavailable to filter from provisioning.
          src_host_node.enabled = false
          src_host_node.save_changes

          scheduled_instances = []
          failed_instances = []
          # TODO: Add interface for prioritize moving instances.
          running_instances_ds = src_host_node.instances_dataset.alives
          running_instances_ds.all.each { |instance|
            begin
              if instance.host_node.status.to_s == 'online'
                db.after_commit do
                  self.job.submit("hva-handle.#{src_host_node.node_id}",
                                  'cleanup', instance.canonical_uuid)
                end
              end

              apply_ha_scheduler(instance)
            rescue Dcmgr::Scheduler::HostNodeSchedulingError => e
              logger.error(e)
              failed_instances << instance
            else
              scheduled_instances << instance
            end
          }
        ensure
          if keep_node_offline == false
            src_host_node.enabled = true
            src_host_node.save_changes
          end
        end

        return {
          :scheduled=>scheduled_instances.map{|i| i.canonical_uuid },
          :failed=>failed_instances.map{|i| i.canonical_uuid }
        }
      rescue ::Exception => e
        logger.error("Error occured. evacuate_from(host_node_id: #{host_node_id}})")
        logger.error(e)
        raise e
      end

      def apply_ha_scheduler(instance)
        svc_type = Dcmgr::Scheduler.service_type(instance)

        if svc_type.host_node_ha.kind_of?(Dcmgr::Scheduler::HostNodeScheduler::AllowOverCommit)
          svc_type.host_node_ha.schedule_over_commit(instance)
        else
          svc_type.host_node_ha.schedule(instance)
        end
        instance.state = Constants::Instance::STATE_PENDING
        instance.save_changes
        logger.info("#{instance.canonical_uuid} is located at #{instance.host_node.canonical_uuid}")

        instance.volumes_dataset.alives.each { |vol|
          if vol.local_volume?
            # move association local_volumes.host_node_id to the new host
            # node.
            if instance.host_node.available_disk_space < vol.size
              raise Dcmgr::Scheduler::HostNodeSchedulingError, "Insufficient local disk space #{vol.canonical_uuid}."
            end
            vol.volume_device.host_node = instance.host_node
            vol.state = Constants::Volume::STATE_PENDING
          else
            # push back the state of shared volume from attached.
            vol.state = Constants::Volume::STATE_AVAILABLE
          end
          vol.save_changes

          logger.info("Migrated local volume association of #{vol.canonical_uuid} to #{vol.volume_device.storage_node.canonical_uuid}.")
        }

        case instance.image.boot_dev_type
        when Const::Image::BOOT_DEV_LOCAL
          db.after_commit do
            # TODO: leave this timer wait out.
            EM.add_timer(5) {
              self.job.submit("local-store-handle.#{instance.host_node.node_id}",
                              'run_local_store', instance.canonical_uuid)
            }
          end
        when Const::Image::BOOT_DEV_SAN
          db.after_commit do
            # TODO: leave this timer wait out.
            EM.add_timer(5) {
              self.job.submit("hva-handle.#{instance.host_node.node_id}",
                              'run_vol_store', instance.canonical_uuid)
            }
          end
        else
          raise "Unknown boot type"
        end

        db.after_commit do
          event.publish('instance.scheduled', :args=>[instance.canonical_uuid])
        end
      end

      protected
      def db
        Sequel::DATABASES.first
      end

      # commit manually before return from the request block
      def commit_transaction
        db << db.__send__(:commit_transaction_sql)
      end

      def rollback_transaction
        db << db.__send__(:rollback_transaction_sql)
      end

      def event
        @event ||= Isono::NodeModules::EventChannel.new(self.node)
      end

      private

      def setup_vif_monitors(instance)
        vif_params = instance.request_params['vifs']
        return if vif_params.nil? || vif_params.empty?

        # Assume that the vif_params and instance.vifs have same order
        # of items.
        viflst = instance.network_vif
        vif_params.each { |name, param|
          vif = viflst.shift
          next if vif.nil?
          if param['monitors'].is_a?(Hash)
            param['monitors'].each { |idx, m|
              vif.add_network_vif_monitor(m.select { |k,v| Models::NetworkVifMonitor.columns.member?(k.to_sym) })
            }
          elsif param['monitors'].is_a?(Array)
            param['monitors'].each { |m|
              vif.add_network_vif_monitor(m.select { |k,v| Models::NetworkVifMonitor.columns.member?(k.to_sym) })
            }
          end
        }
      end

      def init_security_groups(instance)
        instance.network_vif.each { |vnic|
          init_vnic(vnic.canonical_uuid)
        }
      end

    end
  end
end
